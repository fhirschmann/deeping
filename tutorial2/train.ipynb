{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial 2: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.utils.visualize_util import plot\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from callbacks import AUCHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been saved in a file named *data.npz*. It can be loaded using *np.load* and then behaves similar to a Python dictionary, where they keys *records* and *labels* corresponds to a numpy array for our input data and our labels, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved = np.load(\"data.npz\")\n",
    "data = saved[\"records\"]\n",
    "labels = saved[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is a 3-dimensional array, where the first dimension corresponds to the number of instances, the second dimension to the number of time steps, and the third dimension to the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74574, 70, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically, 74574 is the number of drives in our data, 70 the number of records per drive, and 15 features per record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_drives = data.shape[0]\n",
    "n_records = data.shape[1]\n",
    "n_features = data.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the input layer which just takes in our data. It does not contain any neurons other than defining the shape of our input. Since we use the functional API, this also means that all matrix shapes in the following layers will automatically be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(n_records, n_features), name=\"inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add a [Masking](https://keras.io/layers/core/#masking) layer. This layer masks an input sequence by using a mask value to identify timesteps to be skipped. This is needed since we padded our sequence with vectors of zeroes in case we don't have enough observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Masking()(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the fun parts starts. We will add a LSTM layer that summarizes each drive by performing the same computation on a vector of size n_features for each n_records.\n",
    "\n",
    "Remember the unfolding in time computation graph for an RNN\n",
    "\n",
    "![image](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg)\n",
    "\n",
    "where $x$ are the observations for a particular drive, e.g. $x_1$ is the first observation and $x_2$ is the second observation. In our case, we are only interested in the last output $o_n$ where $n = \\text{n_records}$.\n",
    "\n",
    "The LSTM is reset after every drive in n_drives. The output of this LSTM will be a vector of size 20. In other words, the LSTM has 20 neurons in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = LSTM(5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done! Let's wire up the 20 output neurons of the LSTM to just a single output neuron using a [Dense](https://keras.io/layers/core/#dense) layer. A Dense layer is just your regular fully connected NN layer.\n",
    "\n",
    "We will use sigmoid as our activation function because its output lies naturally between 0 and 1, which matches our target well.\n",
    "\n",
    "The output of the dense layer will be $\\sigma(x)$, with $\\sigma(x) = \\frac{1}{1 + exp(-z)}$ where $z$ is just a linear combination of the LSTM output, i.e. $\\sum\\limits w_j x$ of the previous layer, where $w_j$ are the learnt weights from the Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = Dense(1, activation='sigmoid', name='output')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the inputs and outputs of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(input=input, output=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compile our model. Here, we specify two parameters:\n",
    "\n",
    "- optimizer: an optimizer does all the work for us. Given the input and the computed errors, it decides which direction to take. There are quite a few [optimizers available in Keras](https://keras.io/optimizers/)\n",
    "- loss: the loss or objective function tells the model how well we are doing on our data. In our case, this is simply binary crossentropy, but in other cases this may be e.g. mean squared error. Note that this function needs to be differentiable because during training we need to be able to compute the weight updates. Hence, we cannot optimize for e.g. ROCAUC directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a nice summary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "inputs (InputLayer)              (None, 70, 15)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_5 (Masking)              (None, 70, 15)        0           inputs[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 5)             420         masking_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 1)             6           lstm_5[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 426\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that *None* simply means that the model does not really care how many instances we input. The total number of parameters or weights for our model is 2901."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train using a mini-batch size of 30 instances at a time. This speeds up things, because a mini-batch can be computed in parallel on a GPU. We train for three epochs, i.e. we go over our training set three times.\n",
    "\n",
    "Conveniently, Keras will create a hold-out validation set automatically for us when giving the *validation_split* parameter. Let's set it to 20% of our data. Please leave *verbose* at 1 in the following call, otherwise your notebook may freeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59659 samples, validate on 14915 samples\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "model.fit(data, labels, verbose=2, nb_epoch=3, batch_size=30, validation_split=0.2, callbacks=[AUCHistory()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ROCAUC of roughly 60! That's not that great. :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Increase the number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous run, we could see that the loss on our training data did not decrease anymore. The reason for this could be that our model is simply to small to accomodate patterns in our data.\n",
    "\n",
    "Let's try to increase our neurons to 40.\n",
    "\n",
    "For this task, please mark this chunk and select *Cell* and then *Run All Above*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59659 samples, validate on 14915 samples\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch validation AUC: 0.591752913753\n",
      "\n",
      "108s - loss: 0.0535 - val_loss: 0.0279\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch validation AUC: 0.591752913753\n",
      "\n",
      "98s - loss: 0.0314 - val_loss: 0.0280\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch validation AUC: 0.591752913753\n",
      "\n",
      "99s - loss: 0.0313 - val_loss: 0.0281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae2cd76110>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Masking()(input)\n",
    "x = LSTM(100)(x)\n",
    "output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "model = Model(input=input, output=output)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
    "model.fit(data, labels, verbose=2, nb_epoch=3, batch_size=30, validation_split=0.2, callbacks=[AUCHistory()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
