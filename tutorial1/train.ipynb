{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning Tutorial 1: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first deep learning tutorial!\n",
    "\n",
    "In this notebook, we are going to apply neural networks to detect failures of harddrives based on S.M.A.R.T. status observations.\n",
    "\n",
    "Note that you can interrupt the training process at any time by clicking on *Kernel* and then *Interrupt*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [Keras](http://keras.io) framework that abstracts away a lot of the tedious details of deep learning. There are two ways to build neural networks in Keras, the [sequential API](https://keras.io/getting-started/sequential-model-guide/) and the [funcational API](https://keras.io/getting-started/functional-api-guide/)\n",
    "\n",
    "We will only use the funcational API due to its expressive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential API:\n",
    "\n",
    "```Python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "#### Functional API\n",
    "```Python\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this returns a tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# this creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(input=inputs, output=predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same in both APIs\n",
    "\n",
    "```Python\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is the Funcional API better?\n",
    "\n",
    "It allows us to do more, for example when using the functional API we can reuse trained layers and we can train multi input and multi output models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.wrappers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.visualize_util import plot, model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "from callbacks import AUCHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set Information:\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n",
    "\n",
    "\n",
    "Input variables:\n",
    "\n",
    "#### Bank client data:\n",
    "1.  age (numeric)\n",
    "2.  job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5. default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6. housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7. loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "#### related with the last contact of the current campaign:\n",
    "8. contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10. day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "#### other attributes:\n",
    "12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14. previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "#### social and economic context attributes\n",
    "16. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17. cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18. cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20. nr.employed: number of employees - quarterly indicator (numeric)\n",
    "#### Output variable (desired target):\n",
    "21. y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "\n",
    "### Citation:\n",
    "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n",
    "\n",
    "Description and data download location:https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank = pd.read_csv('bank-additional/bank-additional-full.csv', sep=';')\n",
    "bank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 int64\n",
       "job                object\n",
       "marital            object\n",
       "education          object\n",
       "default            object\n",
       "housing            object\n",
       "loan               object\n",
       "contact            object\n",
       "month              object\n",
       "day_of_week        object\n",
       "duration            int64\n",
       "campaign            int64\n",
       "pdays               int64\n",
       "previous            int64\n",
       "poutcome           object\n",
       "emp.var.rate      float64\n",
       "cons.price.idx    float64\n",
       "cons.conf.idx     float64\n",
       "euribor3m         float64\n",
       "nr.employed       float64\n",
       "y                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bank = pd.get_dummies(bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        int64\n",
       "duration                   int64\n",
       "campaign                   int64\n",
       "pdays                      int64\n",
       "previous                   int64\n",
       "emp.var.rate             float64\n",
       "cons.price.idx           float64\n",
       "cons.conf.idx            float64\n",
       "euribor3m                float64\n",
       "nr.employed              float64\n",
       "job_admin.               float64\n",
       "job_blue-collar          float64\n",
       "job_entrepreneur         float64\n",
       "job_housemaid            float64\n",
       "job_management           float64\n",
       "job_retired              float64\n",
       "job_self-employed        float64\n",
       "job_services             float64\n",
       "job_student              float64\n",
       "job_technician           float64\n",
       "job_unemployed           float64\n",
       "job_unknown              float64\n",
       "marital_divorced         float64\n",
       "marital_married          float64\n",
       "marital_single           float64\n",
       "marital_unknown          float64\n",
       "education_basic.4y       float64\n",
       "education_basic.6y       float64\n",
       "education_basic.9y       float64\n",
       "education_high.school    float64\n",
       "                          ...   \n",
       "default_unknown          float64\n",
       "default_yes              float64\n",
       "housing_no               float64\n",
       "housing_unknown          float64\n",
       "housing_yes              float64\n",
       "loan_no                  float64\n",
       "loan_unknown             float64\n",
       "loan_yes                 float64\n",
       "contact_cellular         float64\n",
       "contact_telephone        float64\n",
       "month_apr                float64\n",
       "month_aug                float64\n",
       "month_dec                float64\n",
       "month_jul                float64\n",
       "month_jun                float64\n",
       "month_mar                float64\n",
       "month_may                float64\n",
       "month_nov                float64\n",
       "month_oct                float64\n",
       "month_sep                float64\n",
       "day_of_week_fri          float64\n",
       "day_of_week_mon          float64\n",
       "day_of_week_thu          float64\n",
       "day_of_week_tue          float64\n",
       "day_of_week_wed          float64\n",
       "poutcome_failure         float64\n",
       "poutcome_nonexistent     float64\n",
       "poutcome_success         float64\n",
       "y_no                     float64\n",
       "y_yes                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = bank.drop(['y_no', 'y_yes'], axis=1)\n",
    "Y = bank[['y_no', 'y_yes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is already ordered by time so we can split in trait, validation, and test sets manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24712, 63), (8238, 63), (8238, 63))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:int(0.6*X.shape[0])]\n",
    "X_validation = X[int(0.6*X.shape[0]):int(0.8*X.shape[0])]\n",
    "X_test = X[int(0.8*X.shape[0]):]\n",
    "X_train.shape, X_validation.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24712, 2), (8238, 2), (8238, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = Y[:int(0.6*X.shape[0])]\n",
    "Y_validation = Y[int(0.6*X.shape[0]):int(0.8*X.shape[0])]\n",
    "Y_test = Y[int(0.8*X.shape[0]):]\n",
    "Y_train.shape, Y_validation.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    23524\n",
       "1.0     1188\n",
       "Name: y_yes, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train['y_yes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7326\n",
       "1.0     912\n",
       "Name: y_yes, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_validation['y_yes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    5698\n",
       "1.0    2540\n",
       "Name: y_yes, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test['y_yes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_records = X_train.shape[0]\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the input layer, which just takes in our data. It does not contain any logic other than defining the shape of our input. Since we use the functional API, this also means that all matrix shapes in the following layers will be inferred automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(n_features,), name=\"inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first dimension, *n_records*, is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "        \n",
    "class AUCHistory(keras.callbacks.Callback):\n",
    "    def __init__(self, input_len=1, *args, **kwargs):\n",
    "        self.input_len = input_len\n",
    "        super(AUCHistory, self).__init__(*args, **kwargs)\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.model.validation_data[0])        \n",
    "        auc = roc_auc_score(self.model.validation_data[1][:,1], y_pred[:,1])\n",
    "        print(\"\\nEpoch validation AUC: {}\\n\".format(auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24712 samples, validate on 8238 samples\n",
      "Epoch 1/10\n",
      "24064/24712 [============================>.] - ETA: 0s - loss: 15.3425 - acc: 0.0481\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n",
      "Epoch 2/10\n",
      "24544/24712 [============================>.] - ETA: 0s - loss: 15.3425 - acc: 0.0481\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n",
      "Epoch 3/10\n",
      "24640/24712 [============================>.] - ETA: 0s - loss: 15.3416 - acc: 0.0482\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n",
      "Epoch 4/10\n",
      "23968/24712 [============================>.] - ETA: 0s - loss: 15.3400 - acc: 0.0483\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n",
      "Epoch 5/10\n",
      "23936/24712 [============================>.] - ETA: 0s - loss: 15.3437 - acc: 0.0480\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n",
      "Epoch 6/10\n",
      "24640/24712 [============================>.] - ETA: 0s - loss: 15.3442 - acc: 0.0480\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n",
      "Epoch 7/10\n",
      "23904/24712 [============================>.] - ETA: 0s - loss: 15.3427 - acc: 0.0481\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n",
      "Epoch 8/10\n",
      "23840/24712 [===========================>..] - ETA: 0s - loss: 15.3365 - acc: 0.0485\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n",
      "Epoch 9/10\n",
      "24544/24712 [============================>.] - ETA: 0s - loss: 15.3425 - acc: 0.0481\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n",
      "Epoch 10/10\n",
      "23872/24712 [===========================>..] - ETA: 0s - loss: 15.3464 - acc: 0.0479\n",
      "Epoch validation AUC: 0.5\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 15.3432 - acc: 0.0481 - val_loss: 14.3337 - val_acc: 0.1107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b5eb46b10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs = Input(shape=(n_features,), name=\"inputs\")\n",
    "\n",
    "predictions = Dense(2, activation='softmax')(inputs)\n",
    "\n",
    "# this creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(input=inputs, output=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],)\n",
    "\n",
    "model.fit(X_train.as_matrix(), Y_train.as_matrix(), \n",
    "          validation_data=(X_validation.as_matrix(), Y_validation.as_matrix()), \n",
    "          callbacks=[AUCHistory()])  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24712 samples, validate on 8238 samples\n",
      "Epoch 1/10\n",
      "24256/24712 [============================>.] - ETA: 0s - loss: 0.8122 - acc: 0.9496\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.8109 - acc: 0.9497 - val_loss: 1.8894 - val_acc: 0.8823\n",
      "Epoch 2/10\n",
      "24416/24712 [============================>.] - ETA: 0s - loss: 0.7750 - acc: 0.9519\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.7781 - acc: 0.9517 - val_loss: 1.8894 - val_acc: 0.8823\n",
      "Epoch 3/10\n",
      "24608/24712 [============================>.] - ETA: 0s - loss: 0.7762 - acc: 0.9518\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.7781 - acc: 0.9517 - val_loss: 1.8894 - val_acc: 0.8823\n",
      "Epoch 4/10\n",
      "24512/24712 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.9517\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.7781 - acc: 0.9517 - val_loss: 1.8894 - val_acc: 0.8823\n",
      "Epoch 5/10\n",
      "24704/24712 [============================>.] - ETA: 0s - loss: 0.7777 - acc: 0.9517\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.7781 - acc: 0.9517 - val_loss: 1.8894 - val_acc: 0.8823\n",
      "Epoch 6/10\n",
      "24256/24712 [============================>.] - ETA: 0s - loss: 0.7801 - acc: 0.9516\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.7781 - acc: 0.9517 - val_loss: 1.8894 - val_acc: 0.8823\n",
      "Epoch 7/10\n",
      "24672/24712 [============================>.] - ETA: 0s - loss: 0.7781 - acc: 0.9517\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.7781 - acc: 0.9517 - val_loss: 1.8894 - val_acc: 0.8823\n",
      "Epoch 8/10\n",
      "24672/24712 [============================>.] - ETA: 0s - loss: 0.7774 - acc: 0.9518\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.7781 - acc: 0.9517 - val_loss: 1.8894 - val_acc: 0.8823\n",
      "Epoch 9/10\n",
      "24480/24712 [============================>.] - ETA: 0s - loss: 0.7783 - acc: 0.9517\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.7781 - acc: 0.9517 - val_loss: 1.8894 - val_acc: 0.8823\n",
      "Epoch 10/10\n",
      "24352/24712 [============================>.] - ETA: 0s - loss: 0.7810 - acc: 0.9515\n",
      "Epoch validation AUC: 0.515157352328\n",
      "\n",
      "24712/24712 [==============================] - 0s - loss: 0.7781 - acc: 0.9517 - val_loss: 1.8894 - val_acc: 0.8823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b5efbc050>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(64, activation='relu')(inputs)\n",
    "# x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# this creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(input=inputs, output=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train.as_matrix(), Y_train.as_matrix(), \n",
    "          validation_data=(X_validation.as_matrix(), Y_validation.as_matrix()), \n",
    "          callbacks=[AUCHistory()])  # starts training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the fun parts starts. We will add an LSTM layer that summarizes each drive by performing the same computation on vectors of size n_features for each n_records.\n",
    "\n",
    "Remember the unfolding in time computation graph for an RNN\n",
    "\n",
    "![image](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg)\n",
    "\n",
    "where $x_t$ are observations for a particular drive, e.g. $x_1$ is the first observation and $x_2$ is the second observation. In our case, we are only interested in the last output $o_t$ where $t = \\text{n_records}$. The output of this LSTM will be a vector of size 10. In other words, the LSTM has 10 neurons in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = LSTM(10)(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done! Let's wire up the 10 output neurons of the LSTM to just a single output neuron using a [Dense](https://keras.io/layers/core/#dense) layer. A Dense layer is just your regular fully connected NN layer.\n",
    "\n",
    "The output of the dense layer will be $\\sigma(x)$, with $\\sigma(x) = \\frac{1}{1 + exp(-z)}$ where $z$ is just a linear combination of the LSTM outputs, i.e. $\\sum\\limits w_j x_j$ of the previous layer, where $w_j$ are the learnt weights  for the connection from the LSTM to the Dense layer and $x_j$ is the output of the LSTM. Conveniently, the output of $\\sigma(x)$ lies between $0$ and $1$ and matches our target well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = Dense(1, activation='sigmoid', name='output')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap up the input and output of our Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(input=input, output=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compile our model. Here, we specify two parameters:\n",
    "\n",
    "- optimizer: an optimizer does all the work for us. Given the input and the computed errors, it decides which direction to take. There are quite a few [optimizers available in Keras](https://keras.io/optimizers/).\n",
    "- loss: the loss or objective function tells the model how well we are doing on our data. In our case, this is simply binary crossentropy, but in other cases this may be e.g. mean squared error. Note that this function needs to be differentiable because during training we need to be able to compute the weight updates. Hence, we cannot optimize for e.g. ROCAUC directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Nadam(), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a nice plot of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that *None* simply means that the model does not really care how many instances we input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train using a mini-batch size of 20 instances at a time. This speeds up things, because a mini-batch can be computed in parallel on a GPU. We train for eight epochs, i.e. we go over our training set three times.\n",
    "\n",
    "Conveniently, Keras will create a hold-out validation set automatically for us when giving the *validation_split* parameter. Let's set it to 20% of our data. **Please leave *verbose* at 2** in the following call, otherwise your notebook may freeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(data, labels, verbose=2, nb_epoch=8, batch_size=20, validation_split=0.2, callbacks=[AUCHistory()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't so bad. You can see that our network converges after the second epoch and neither our training loss nor the AUC improve anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Increase the number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the model is simply to small to accomodate patterns in our data? Let's try to increase our neurons to 50.\n",
    "\n",
    "**Your task** is to:\n",
    "\n",
    "- Mark this chunk and select *Cell* and then *Run All Above*\n",
    "- Increase the number of neurons to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(n_records, n_features), name=\"inputs\")\n",
    "x = input\n",
    "x = LSTM(20)(x)\n",
    "output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "model1 = Model(input=input, output=output)\n",
    "model1.compile(optimizer=Nadam(), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.fit(data, labels, verbose=2, nb_epoch=8, batch_size=20, validation_split=0.2, callbacks=[AUCHistory()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! If you want, you can try a different number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Add another layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our model deeper! This is deep learning after all. Note that our network is already deep in time, i.e. we take into consideration 90 time steps. But we can also make it deeper vertically.\n",
    "\n",
    "Your task is to\n",
    "\n",
    "- stack another LSTM layer on top of the layer we already have.\n",
    "\n",
    "At each time step, the first LSTM will feed into the second LSTM. This is called stacking.\n",
    "\n",
    "Note that for this, you have to set *return_sequences=True* in the first LSTM. Do you understand why this is required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(n_records, n_features), name=\"inputs\")\n",
    "x = input\n",
    "\n",
    "### Your code goes here:\n",
    "x = LSTM(30, return_sequences=True)(x)\n",
    "x = LSTM(30)(x)\n",
    "##\n",
    "\n",
    "output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "model2 = Model(input=input, output=output)\n",
    "model2.compile(optimizer=Nadam(), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model2, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.fit(data, labels, verbose=2, nb_epoch=8, batch_size=20, validation_split=0.2, callbacks=[AUCHistory()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Change the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be a bit more challenging. We are going to use an additional attribute: the disk's model. However, because this attribute is constant in a disks's time series, we will not add it to LSTM that summarizes the time series.\n",
    "\n",
    "Instead, we will *merge* the $n$-dimensional vector output of the LSTM with a $m$-dimensional vector, where merging means concatenating the two vectors into a vector of dimensionality $n+m$.\n",
    "\n",
    "Conveniently, we have already encoded the disk's model as a one-hot vector, i.e. the columns in the following matrix correspond to unique disk models and the rows to individual disks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = saved[\"models\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you have to do the following:\n",
    "\n",
    "- Create a second Input with shape `(number_of_models, )`. Note that you do not need masking here because we are at this step no longer working with a time series with missing observations.\n",
    "- Introduce a [Merge](https://keras.io/getting-started/sequential-model-guide/#the-merge-layer) layer that merges `[x, your_new_input]`\n",
    "- Modify the `Model` instantiation to take two inputs simultaneously, similar to what you have done in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(n_records, n_features), name=\"inputs\")\n",
    "input2 = Input(shape=(models.shape[1], ))\n",
    "\n",
    "x = input\n",
    "x = LSTM(20, return_sequences=True)(x)\n",
    "x = LSTM(20)(x)\n",
    "\n",
    "x2 = input2\n",
    "\n",
    "x = merge([x, x2], mode=\"concat\")\n",
    "\n",
    "output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "model3 = Model(input=[input, input2], output=output)\n",
    "model3.compile(optimizer=Nadam(), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model3, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3.fit([data, models], labels, verbose=2, nb_epoch=10, batch_size=20, validation_split=0.2, callbacks=[AUCHistory()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still getting better! This concludes the second deep learning tutorial.\n",
    "\n",
    "If you still have some free time, you are welcome to experiment further with our architecture. Things you may want to try:\n",
    "\n",
    "- Use GRU instead of LSTM units\n",
    "- Introduce regularization such as dropout"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
